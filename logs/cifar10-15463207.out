*** WARNING: This is an experimental installation of pytorch built specifically for MPI / distributed running ***
 4: 2018-10-04 18:17:50,067 INFO Initializing
13: 2018-10-04 18:17:50,072 INFO Initializing
 7: 2018-10-04 18:17:50,075 INFO Initializing
12: 2018-10-04 18:17:50,076 INFO Initializing
 2: 2018-10-04 18:17:50,076 INFO Initializing
14: 2018-10-04 18:17:50,080 INFO Initializing
 0: 2018-10-04 18:17:50,082 INFO Initializing
11: 2018-10-04 18:17:50,083 INFO Initializing
15: 2018-10-04 18:17:50,086 INFO Initializing
 6: 2018-10-04 18:17:50,087 INFO Initializing
 1: 2018-10-04 18:17:50,087 INFO Initializing
 5: 2018-10-04 18:17:50,088 INFO Initializing
10: 2018-10-04 18:17:50,410 INFO Initializing
 8: 2018-10-04 18:17:50,410 INFO Initializing
 9: 2018-10-04 18:17:50,424 INFO Initializing
 3: 2018-10-04 18:17:50,425 INFO Initializing
 0: 2018-10-04 18:17:50,473 INFO MPI rank 0
 6: 2018-10-04 18:17:50,472 INFO MPI rank 6
10: 2018-10-04 18:17:50,471 INFO MPI rank 10
12: 2018-10-04 18:17:50,472 INFO MPI rank 12
11: 2018-10-04 18:17:50,473 INFO MPI rank 11
14: 2018-10-04 18:17:50,472 INFO MPI rank 14
 2: 2018-10-04 18:17:50,471 INFO MPI rank 2
 4: 2018-10-04 18:17:50,471 INFO MPI rank 4
 7: 2018-10-04 18:17:50,472 INFO MPI rank 7
 8: 2018-10-04 18:17:50,471 INFO MPI rank 8
 1: 2018-10-04 18:17:50,472 INFO MPI rank 1
15: 2018-10-04 18:17:50,471 INFO MPI rank 15
 3: 2018-10-04 18:17:50,472 INFO MPI rank 3
13: 2018-10-04 18:17:50,472 INFO MPI rank 13
 5: 2018-10-04 18:17:50,472 INFO MPI rank 5
 9: 2018-10-04 18:17:50,472 INFO MPI rank 9
 0: 2018-10-04 18:17:50,477 INFO Configuration: {'data_config': {'name': 'cifar10', 'data_path': '$SCRATCH/pytorch-cifar10/data', 'n_train': 16384, 'n_valid': 10000}, 'experiment_config': {'name': 'cifar10', 'output_dir': '$SCRATCH/pytorch-cifar10/output'}, 'model_config': {'model_type': 'resnet50_cifar10', 'optimizer': 'Adam', 'learning_rate': 0.001}, 'train_config': {'batch_size': 128, 'n_epochs': 1}}
15: 2018-10-04 18:17:53,904 INFO Loaded 16384 training samples and 10000 validation samples
 6: 2018-10-04 18:17:53,909 INFO Loaded 16384 training samples and 10000 validation samples
10: 2018-10-04 18:17:53,916 INFO Loaded 16384 training samples and 10000 validation samples
 2: 2018-10-04 18:17:53,917 INFO Loaded 16384 training samples and 10000 validation samples
 9: 2018-10-04 18:17:53,919 INFO Loaded 16384 training samples and 10000 validation samples
13: 2018-10-04 18:17:53,924 INFO Loaded 16384 training samples and 10000 validation samples
 7: 2018-10-04 18:17:53,924 INFO Loaded 16384 training samples and 10000 validation samples
 0: 2018-10-04 18:17:53,931 INFO Loaded 16384 training samples and 10000 validation samples
 3: 2018-10-04 18:17:53,934 INFO Loaded 16384 training samples and 10000 validation samples
14: 2018-10-04 18:17:53,934 INFO Loaded 16384 training samples and 10000 validation samples
11: 2018-10-04 18:17:53,946 INFO Loaded 16384 training samples and 10000 validation samples
 1: 2018-10-04 18:17:53,946 INFO Loaded 16384 training samples and 10000 validation samples
 8: 2018-10-04 18:17:53,948 INFO Loaded 16384 training samples and 10000 validation samples
 5: 2018-10-04 18:17:53,957 INFO Loaded 16384 training samples and 10000 validation samples
 4: 2018-10-04 18:17:53,958 INFO Loaded 16384 training samples and 10000 validation samples
12: 2018-10-04 18:17:53,960 INFO Loaded 16384 training samples and 10000 validation samples
 1: 2018-10-04 18:17:54,221 INFO Epoch 0
 4: 2018-10-04 18:17:54,220 INFO Epoch 0
 6: 2018-10-04 18:17:54,221 INFO Epoch 0
10: 2018-10-04 18:17:54,220 INFO Epoch 0
 7: 2018-10-04 18:17:54,220 INFO Epoch 0
 5: 2018-10-04 18:17:54,221 INFO Epoch 0
11: 2018-10-04 18:17:54,222 INFO Epoch 0
14: 2018-10-04 18:17:54,221 INFO Epoch 0
15: 2018-10-04 18:17:54,220 INFO Epoch 0
 2: 2018-10-04 18:17:54,220 INFO Epoch 0
 3: 2018-10-04 18:17:54,221 INFO Epoch 0
13: 2018-10-04 18:17:54,221 INFO Epoch 0
 8: 2018-10-04 18:17:54,220 INFO Epoch 0
12: 2018-10-04 18:17:54,221 INFO Epoch 0
 9: 2018-10-04 18:17:54,221 INFO Epoch 0
 0: 2018-10-04 18:17:54,223 INFO Model: 
 0: DistributedDataParallelCPU(
 0:   (module): ResNet(
 0:     (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:     (layer1): Sequential(
 0:       (0): Bottleneck(
 0:         (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential(
 0:           (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:           (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         )
 0:       )
 0:       (1): Bottleneck(
 0:         (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (2): Bottleneck(
 0:         (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:     )
 0:     (layer2): Sequential(
 0:       (0): Bottleneck(
 0:         (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential(
 0:           (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
 0:           (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         )
 0:       )
 0:       (1): Bottleneck(
 0:         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (2): Bottleneck(
 0:         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (3): Bottleneck(
 0:         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:     )
 0:     (layer3): Sequential(
 0:       (0): Bottleneck(
 0:         (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential(
 0:           (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
 0:           (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         )
 0:       )
 0:       (1): Bottleneck(
 0:         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (2): Bottleneck(
 0:         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (3): Bottleneck(
 0:         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (4): Bottleneck(
 0:         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (5): Bottleneck(
 0:         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:     )
 0:     (layer4): Sequential(
 0:       (0): Bottleneck(
 0:         (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential(
 0:           (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
 0:           (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         )
 0:       )
 0:       (1): Bottleneck(
 0:         (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (2): Bottleneck(
 0:         (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:     )
 0:     (linear): Linear(in_features=2048, out_features=10, bias=True)
 0:   )
 0: )
 0: Parameters: 23520842
 0: 2018-10-04 18:17:54,223 INFO Epoch 0
14: 2018-10-04 18:18:50,890 INFO   Training loss: 2.327
15: 2018-10-04 18:18:50,890 INFO   Training loss: 2.416
 3: 2018-10-04 18:18:50,891 INFO   Training loss: 2.314
12: 2018-10-04 18:18:50,891 INFO   Training loss: 2.300
 0: 2018-10-04 18:18:50,891 INFO   Training loss: 2.338
 4: 2018-10-04 18:18:50,890 INFO   Training loss: 2.326
10: 2018-10-04 18:18:50,890 INFO   Training loss: 2.331
13: 2018-10-04 18:18:50,891 INFO   Training loss: 2.347
 1: 2018-10-04 18:18:50,892 INFO   Training loss: 2.323
 6: 2018-10-04 18:18:50,893 INFO   Training loss: 2.378
 9: 2018-10-04 18:18:50,894 INFO   Training loss: 2.347
 7: 2018-10-04 18:18:50,893 INFO   Training loss: 2.335
 5: 2018-10-04 18:18:50,894 INFO   Training loss: 2.346
11: 2018-10-04 18:18:50,895 INFO   Training loss: 2.316
 2: 2018-10-04 18:18:50,894 INFO   Training loss: 2.337
 8: 2018-10-04 18:18:50,894 INFO   Training loss: 2.304
11: 2018-10-04 18:20:26,479 INFO   Validation loss: 2.347 acc: 0.100
11: 2018-10-04 18:20:26,643 INFO Finished training
11: 2018-10-04 18:20:26,644 INFO Train samples 1024 time 56.6728s rate 18.0686 samples/s
11: 2018-10-04 18:20:26,644 INFO Valid rate: 104.622 samples/s
11: 2018-10-04 18:20:26,644 INFO All done!
11: Files already downloaded and verified
11: Files already downloaded and verified
10: 2018-10-04 18:20:27,618 INFO   Validation loss: 2.347 acc: 0.100
15: 2018-10-04 18:20:27,892 INFO   Validation loss: 2.348 acc: 0.100
 1: 2018-10-04 18:20:28,018 INFO   Validation loss: 2.346 acc: 0.126
14: 2018-10-04 18:20:28,237 INFO   Validation loss: 2.347 acc: 0.100
10: 2018-10-04 18:20:28,377 INFO Finished training
10: 2018-10-04 18:20:28,378 INFO Train samples 1024 time 56.6697s rate 18.0696 samples/s
10: 2018-10-04 18:20:28,378 INFO Valid rate: 103.384 samples/s
10: 2018-10-04 18:20:28,378 INFO All done!
10: Files already downloaded and verified
10: Files already downloaded and verified
 0: 2018-10-04 18:20:28,399 INFO   Validation loss: 2.350 acc: 0.100
 3: 2018-10-04 18:20:28,495 INFO   Validation loss: 2.346 acc: 0.100
15: 2018-10-04 18:20:28,980 INFO Finished training
15: 2018-10-04 18:20:28,980 INFO Train samples 1024 time 56.6693s rate 18.0698 samples/s
15: 2018-10-04 18:20:28,980 INFO Valid rate: 103.092 samples/s
15: 2018-10-04 18:20:28,980 INFO All done!
15: Files already downloaded and verified
15: Files already downloaded and verified
 1: 2018-10-04 18:20:29,784 INFO Finished training
 1: 2018-10-04 18:20:29,784 INFO Train samples 1024 time 56.6703s rate 18.0694 samples/s
 1: 2018-10-04 18:20:29,784 INFO Valid rate: 102.985 samples/s
 1: 2018-10-04 18:20:29,784 INFO All done!
 1: Files already downloaded and verified
 1: Files already downloaded and verified
 3: 2018-10-04 18:20:30,669 INFO Finished training
 3: 2018-10-04 18:20:30,669 INFO Train samples 1024 time 56.6694s rate 18.0697 samples/s
 3: 2018-10-04 18:20:30,669 INFO Valid rate: 102.455 samples/s
 3: 2018-10-04 18:20:30,669 INFO All done!
 3: Files already downloaded and verified
 3: Files already downloaded and verified
 9: 2018-10-04 18:20:31,616 INFO   Validation loss: 2.348 acc: 0.100
14: 2018-10-04 18:20:32,238 INFO Finished training
14: 2018-10-04 18:20:32,238 INFO Train samples 1024 time 56.6692s rate 18.0698 samples/s
14: 2018-10-04 18:20:32,238 INFO Valid rate: 102.727 samples/s
14: 2018-10-04 18:20:32,238 INFO All done!
14: Files already downloaded and verified
14: Files already downloaded and verified
 0: 2018-10-04 18:20:33,668 INFO Saving summaries to /global/cscratch1/sd/sfarrell/pytorch-cifar10/output/summaries.npz
 0: 2018-10-04 18:20:33,682 INFO Finished training
 0: 2018-10-04 18:20:33,682 INFO Train samples 1024 time 56.6676s rate 18.0703 samples/s
 0: 2018-10-04 18:20:33,683 INFO Valid rate: 102.558 samples/s
 0: 2018-10-04 18:20:33,683 INFO All done!
 0: Files already downloaded and verified
 0: Files already downloaded and verified
 9: 2018-10-04 18:20:33,848 INFO Finished training
 9: 2018-10-04 18:20:33,848 INFO Train samples 1024 time 56.6721s rate 18.0689 samples/s
 9: 2018-10-04 18:20:33,848 INFO Valid rate: 99.2942 samples/s
 9: 2018-10-04 18:20:33,849 INFO All done!
 9: Files already downloaded and verified
 9: Files already downloaded and verified
13: 2018-10-04 18:20:34,279 INFO   Validation loss: 2.349 acc: 0.100
13: 2018-10-04 18:20:34,516 INFO Finished training
13: 2018-10-04 18:20:34,516 INFO Train samples 1024 time 56.67s rate 18.0695 samples/s
13: 2018-10-04 18:20:34,516 INFO Valid rate: 96.7241 samples/s
13: 2018-10-04 18:20:34,517 INFO All done!
13: Files already downloaded and verified
13: Files already downloaded and verified
 7: 2018-10-04 18:20:40,517 INFO   Validation loss: 2.350 acc: 0.100
 7: 2018-10-04 18:20:40,839 INFO Finished training
 7: 2018-10-04 18:20:40,839 INFO Train samples 1024 time 56.6724s rate 18.0688 samples/s
 7: 2018-10-04 18:20:40,839 INFO Valid rate: 91.2267 samples/s
 7: 2018-10-04 18:20:40,839 INFO All done!
 7: Files already downloaded and verified
 7: Files already downloaded and verified
 4: 2018-10-04 18:20:46,205 INFO   Validation loss: 2.346 acc: 0.100
 4: 2018-10-04 18:20:46,539 INFO Finished training
 4: 2018-10-04 18:20:46,539 INFO Train samples 1024 time 56.6697s rate 18.0696 samples/s
 4: 2018-10-04 18:20:46,539 INFO Valid rate: 86.7195 samples/s
 4: 2018-10-04 18:20:46,539 INFO All done!
 4: Files already downloaded and verified
 4: Files already downloaded and verified
12: 2018-10-04 18:20:46,839 INFO   Validation loss: 2.346 acc: 0.100
12: 2018-10-04 18:20:47,162 INFO Finished training
12: 2018-10-04 18:20:47,163 INFO Train samples 1024 time 56.6694s rate 18.0697 samples/s
12: 2018-10-04 18:20:47,163 INFO Valid rate: 86.2462 samples/s
12: 2018-10-04 18:20:47,163 INFO All done!
12: Files already downloaded and verified
12: Files already downloaded and verified
 5: 2018-10-04 18:20:47,189 INFO   Validation loss: 2.344 acc: 0.127
 5: 2018-10-04 18:20:47,497 INFO Finished training
 5: 2018-10-04 18:20:47,498 INFO Train samples 1024 time 56.6725s rate 18.0687 samples/s
 5: 2018-10-04 18:20:47,498 INFO Valid rate: 85.9889 samples/s
 5: 2018-10-04 18:20:47,498 INFO All done!
 5: Files already downloaded and verified
 5: Files already downloaded and verified
 2: 2018-10-04 18:20:48,693 INFO   Validation loss: 2.348 acc: 0.100
 2: 2018-10-04 18:20:49,009 INFO Finished training
 2: 2018-10-04 18:20:49,009 INFO Train samples 1024 time 56.6728s rate 18.0686 samples/s
 2: 2018-10-04 18:20:49,009 INFO Valid rate: 84.8909 samples/s
 2: 2018-10-04 18:20:49,009 INFO All done!
 2: Files already downloaded and verified
 2: Files already downloaded and verified
 6: 2018-10-04 18:21:03,150 INFO   Validation loss: 2.347 acc: 0.100
 6: 2018-10-04 18:21:03,524 INFO Finished training
 6: 2018-10-04 18:21:03,524 INFO Train samples 1024 time 56.6719s rate 18.0689 samples/s
 6: 2018-10-04 18:21:03,524 INFO Valid rate: 75.6113 samples/s
 6: 2018-10-04 18:21:03,524 INFO All done!
 6: Files already downloaded and verified
 6: Files already downloaded and verified
 8: 2018-10-04 18:21:04,296 INFO   Validation loss: 2.348 acc: 0.100
 8: 2018-10-04 18:21:04,528 INFO Finished training
 8: 2018-10-04 18:21:04,528 INFO Train samples 1024 time 56.673s rate 18.0686 samples/s
 8: 2018-10-04 18:21:04,528 INFO Valid rate: 74.9714 samples/s
 8: 2018-10-04 18:21:04,528 INFO All done!
 8: Files already downloaded and verified
 8: Files already downloaded and verified
