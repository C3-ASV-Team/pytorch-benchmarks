*** WARNING: This is an experimental installation of pytorch built specifically for MPI / distributed running ***
 4: 2018-10-07 21:40:52,592 INFO Initializing
 1: 2018-10-07 21:40:52,597 INFO Initializing
 2: 2018-10-07 21:40:52,594 INFO Initializing
 0: 2018-10-07 21:40:52,598 INFO Initializing
 5: 2018-10-07 21:40:52,600 INFO Initializing
 3: 2018-10-07 21:40:52,615 INFO Initializing
15: 2018-10-07 21:40:52,621 INFO Initializing
14: 2018-10-07 21:40:52,626 INFO Initializing
 7: 2018-10-07 21:40:52,625 INFO Initializing
11: 2018-10-07 21:40:52,622 INFO Initializing
 6: 2018-10-07 21:40:52,626 INFO Initializing
13: 2018-10-07 21:40:52,629 INFO Initializing
 8: 2018-10-07 21:40:52,635 INFO Initializing
 9: 2018-10-07 21:40:52,641 INFO Initializing
12: 2018-10-07 21:40:52,647 INFO Initializing
10: 2018-10-07 21:40:52,653 INFO Initializing
 4: 2018-10-07 21:40:52,697 INFO MPI rank 4
10: 2018-10-07 21:40:52,699 INFO MPI rank 10
 0: 2018-10-07 21:40:52,697 INFO MPI rank 0
 8: 2018-10-07 21:40:52,700 INFO MPI rank 8
 6: 2018-10-07 21:40:52,698 INFO MPI rank 6
14: 2018-10-07 21:40:52,700 INFO MPI rank 14
12: 2018-10-07 21:40:52,699 INFO MPI rank 12
 2: 2018-10-07 21:40:52,699 INFO MPI rank 2
 7: 2018-10-07 21:40:52,699 INFO MPI rank 7
 1: 2018-10-07 21:40:52,702 INFO MPI rank 1
 9: 2018-10-07 21:40:52,700 INFO MPI rank 9
 5: 2018-10-07 21:40:52,698 INFO MPI rank 5
 3: 2018-10-07 21:40:52,702 INFO MPI rank 3
11: 2018-10-07 21:40:52,696 INFO MPI rank 11
15: 2018-10-07 21:40:52,697 INFO MPI rank 15
13: 2018-10-07 21:40:52,696 INFO MPI rank 13
 0: 2018-10-07 21:40:52,702 INFO Configuration: {'data_config': {'name': 'cifar10', 'data_path': '$SCRATCH/pytorch-cifar10/data', 'n_train': 32768, 'n_valid': 8192}, 'experiment_config': {'name': 'cifar10', 'output_dir': '$SCRATCH/pytorch-cifar10/output'}, 'model_config': {'model_type': 'resnet50_cifar10', 'optimizer': 'Adam', 'learning_rate': 0.001}, 'train_config': {'batch_size': 64, 'n_epochs': 1}}
 7: 2018-10-07 21:40:55,465 INFO Loaded 32768 training samples and 8192 validation samples
14: 2018-10-07 21:40:55,469 INFO Loaded 32768 training samples and 8192 validation samples
 1: 2018-10-07 21:40:55,482 INFO Loaded 32768 training samples and 8192 validation samples
13: 2018-10-07 21:40:55,478 INFO Loaded 32768 training samples and 8192 validation samples
 4: 2018-10-07 21:40:55,484 INFO Loaded 32768 training samples and 8192 validation samples
11: 2018-10-07 21:40:55,490 INFO Loaded 32768 training samples and 8192 validation samples
12: 2018-10-07 21:40:55,494 INFO Loaded 32768 training samples and 8192 validation samples
10: 2018-10-07 21:40:55,495 INFO Loaded 32768 training samples and 8192 validation samples
 5: 2018-10-07 21:40:55,506 INFO Loaded 32768 training samples and 8192 validation samples
 3: 2018-10-07 21:40:55,511 INFO Loaded 32768 training samples and 8192 validation samples
 0: 2018-10-07 21:40:55,516 INFO Loaded 32768 training samples and 8192 validation samples
 9: 2018-10-07 21:40:55,521 INFO Loaded 32768 training samples and 8192 validation samples
 2: 2018-10-07 21:40:55,521 INFO Loaded 32768 training samples and 8192 validation samples
 6: 2018-10-07 21:40:55,529 INFO Loaded 32768 training samples and 8192 validation samples
15: 2018-10-07 21:40:55,532 INFO Loaded 32768 training samples and 8192 validation samples
 8: 2018-10-07 21:40:55,540 INFO Loaded 32768 training samples and 8192 validation samples
 4: 2018-10-07 21:40:55,803 INFO Epoch 0
 7: 2018-10-07 21:40:55,805 INFO Epoch 0
 1: 2018-10-07 21:40:55,807 INFO Epoch 0
 9: 2018-10-07 21:40:55,806 INFO Epoch 0
 5: 2018-10-07 21:40:55,803 INFO Epoch 0
 6: 2018-10-07 21:40:55,804 INFO Epoch 0
10: 2018-10-07 21:40:55,805 INFO Epoch 0
 3: 2018-10-07 21:40:55,807 INFO Epoch 0
11: 2018-10-07 21:40:55,802 INFO Epoch 0
 8: 2018-10-07 21:40:55,806 INFO Epoch 0
14: 2018-10-07 21:40:55,806 INFO Epoch 0
12: 2018-10-07 21:40:55,804 INFO Epoch 0
15: 2018-10-07 21:40:55,802 INFO Epoch 0
 2: 2018-10-07 21:40:55,804 INFO Epoch 0
13: 2018-10-07 21:40:55,802 INFO Epoch 0
 0: 2018-10-07 21:40:55,805 INFO Model: 
 0: DistributedDataParallelCPU(
 0:   (module): ResNet(
 0:     (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:     (layer1): Sequential(
 0:       (0): Bottleneck(
 0:         (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential(
 0:           (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:           (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         )
 0:       )
 0:       (1): Bottleneck(
 0:         (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (2): Bottleneck(
 0:         (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:     )
 0:     (layer2): Sequential(
 0:       (0): Bottleneck(
 0:         (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential(
 0:           (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
 0:           (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         )
 0:       )
 0:       (1): Bottleneck(
 0:         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (2): Bottleneck(
 0:         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (3): Bottleneck(
 0:         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:     )
 0:     (layer3): Sequential(
 0:       (0): Bottleneck(
 0:         (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential(
 0:           (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
 0:           (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         )
 0:       )
 0:       (1): Bottleneck(
 0:         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (2): Bottleneck(
 0:         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (3): Bottleneck(
 0:         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (4): Bottleneck(
 0:         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (5): Bottleneck(
 0:         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:     )
 0:     (layer4): Sequential(
 0:       (0): Bottleneck(
 0:         (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential(
 0:           (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
 0:           (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         )
 0:       )
 0:       (1): Bottleneck(
 0:         (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:       (2): Bottleneck(
 0:         (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 0:         (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
 0:         (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 0:         (shortcut): Sequential()
 0:       )
 0:     )
 0:     (linear): Linear(in_features=2048, out_features=10, bias=True)
 0:   )
 0: )
 0: Parameters: 23520842
 0: 2018-10-07 21:40:55,805 INFO Epoch 0
 8: 2018-10-07 21:42:37,282 INFO   Training loss: 1.837
 9: 2018-10-07 21:42:37,282 INFO   Training loss: 1.895
13: 2018-10-07 21:42:37,278 INFO   Training loss: 1.859
 0: 2018-10-07 21:42:37,279 INFO   Training loss: 1.878
 2: 2018-10-07 21:42:37,281 INFO   Training loss: 1.827
10: 2018-10-07 21:42:37,281 INFO   Training loss: 1.925
15: 2018-10-07 21:42:37,279 INFO   Training loss: 1.891
11: 2018-10-07 21:42:37,279 INFO   Training loss: 1.880
 3: 2018-10-07 21:42:37,284 INFO   Training loss: 1.850
 6: 2018-10-07 21:42:37,281 INFO   Training loss: 1.834
14: 2018-10-07 21:42:37,284 INFO   Training loss: 1.902
 4: 2018-10-07 21:42:37,282 INFO   Training loss: 1.790
12: 2018-10-07 21:42:37,284 INFO   Training loss: 1.870
 7: 2018-10-07 21:42:37,285 INFO   Training loss: 1.862
 5: 2018-10-07 21:42:37,283 INFO   Training loss: 1.830
 1: 2018-10-07 21:42:37,287 INFO   Training loss: 1.901
 5: 2018-10-07 21:44:01,201 INFO   Validation loss: 3.577 acc: 0.107
 5: 2018-10-07 21:44:01,201 INFO Finished training
 5: 2018-10-07 21:44:01,202 INFO Train samples 2048 time 101.479s rate 20.1814 samples/s
 5: 2018-10-07 21:44:01,202 INFO Valid rate: 97.6275 samples/s
 5: 2018-10-07 21:44:01,202 INFO All done!
 5: Files already downloaded and verified
 5: Files already downloaded and verified
11: 2018-10-07 21:44:01,406 INFO   Validation loss: 3.571 acc: 0.108
11: 2018-10-07 21:44:01,406 INFO Finished training
11: 2018-10-07 21:44:01,407 INFO Train samples 2048 time 101.476s rate 20.1821 samples/s
11: 2018-10-07 21:44:01,407 INFO Valid rate: 97.3813 samples/s
11: 2018-10-07 21:44:01,407 INFO All done!
11: Files already downloaded and verified
11: Files already downloaded and verified
12: 2018-10-07 21:44:01,554 INFO   Validation loss: 3.536 acc: 0.108
12: 2018-10-07 21:44:01,554 INFO Finished training
12: 2018-10-07 21:44:01,554 INFO Train samples 2048 time 101.479s rate 20.1815 samples/s
12: 2018-10-07 21:44:01,555 INFO Valid rate: 97.2179 samples/s
12: 2018-10-07 21:44:01,555 INFO All done!
12: Files already downloaded and verified
12: Files already downloaded and verified
 7: 2018-10-07 21:44:01,979 INFO   Validation loss: 3.586 acc: 0.107
 7: 2018-10-07 21:44:01,979 INFO Finished training
 7: 2018-10-07 21:44:01,979 INFO Train samples 2048 time 101.479s rate 20.1815 samples/s
 7: 2018-10-07 21:44:01,980 INFO Valid rate: 96.7254 samples/s
 7: 2018-10-07 21:44:01,980 INFO All done!
 9: 2018-10-07 21:44:01,983 INFO   Validation loss: 3.568 acc: 0.108
 9: 2018-10-07 21:44:01,984 INFO Finished training
 9: 2018-10-07 21:44:01,984 INFO Train samples 2048 time 101.476s rate 20.1822 samples/s
 9: 2018-10-07 21:44:01,984 INFO Valid rate: 96.724 samples/s
 9: 2018-10-07 21:44:01,984 INFO All done!
 7: Files already downloaded and verified
 7: Files already downloaded and verified
 9: Files already downloaded and verified
 9: Files already downloaded and verified
15: 2018-10-07 21:44:02,063 INFO   Validation loss: 3.602 acc: 0.107
15: 2018-10-07 21:44:02,064 INFO Finished training
15: 2018-10-07 21:44:02,064 INFO Train samples 2048 time 101.476s rate 20.1821 samples/s
15: 2018-10-07 21:44:02,064 INFO Valid rate: 96.6318 samples/s
15: 2018-10-07 21:44:02,064 INFO All done!
15: Files already downloaded and verified
15: Files already downloaded and verified
 4: 2018-10-07 21:44:02,207 INFO   Validation loss: 3.505 acc: 0.108
 4: 2018-10-07 21:44:02,207 INFO Finished training
 4: 2018-10-07 21:44:02,207 INFO Train samples 2048 time 101.479s rate 20.1815 samples/s
 4: 2018-10-07 21:44:02,207 INFO Valid rate: 96.4637 samples/s
 4: 2018-10-07 21:44:02,207 INFO All done!
 4: Files already downloaded and verified
 4: Files already downloaded and verified
 1: 2018-10-07 21:44:02,320 INFO   Validation loss: 3.534 acc: 0.108
 1: 2018-10-07 21:44:02,320 INFO Finished training
 1: 2018-10-07 21:44:02,320 INFO Train samples 2048 time 101.48s rate 20.1814 samples/s
 1: 2018-10-07 21:44:02,320 INFO Valid rate: 96.3474 samples/s
 1: 2018-10-07 21:44:02,320 INFO All done!
 1: Files already downloaded and verified
 1: Files already downloaded and verified
14: 2018-10-07 21:44:02,622 INFO   Validation loss: 3.614 acc: 0.107
14: 2018-10-07 21:44:02,622 INFO Finished training
14: 2018-10-07 21:44:02,623 INFO Train samples 2048 time 101.477s rate 20.1819 samples/s
14: 2018-10-07 21:44:02,623 INFO Valid rate: 95.9954 samples/s
14: 2018-10-07 21:44:02,623 INFO All done!
14: Files already downloaded and verified
14: Files already downloaded and verified
 8: 2018-10-07 21:44:02,772 INFO   Validation loss: 3.521 acc: 0.108
 8: 2018-10-07 21:44:02,772 INFO Finished training
 8: 2018-10-07 21:44:02,772 INFO Train samples 2048 time 101.475s rate 20.1822 samples/s
 8: 2018-10-07 21:44:02,772 INFO Valid rate: 95.8251 samples/s
 8: 2018-10-07 21:44:02,772 INFO All done!
 8: Files already downloaded and verified
 8: Files already downloaded and verified
10: 2018-10-07 21:44:02,858 INFO   Validation loss: 3.586 acc: 0.107
10: 2018-10-07 21:44:02,858 INFO Finished training
10: 2018-10-07 21:44:02,858 INFO Train samples 2048 time 101.476s rate 20.1821 samples/s
10: 2018-10-07 21:44:02,859 INFO Valid rate: 95.7284 samples/s
10: 2018-10-07 21:44:02,859 INFO All done!
13: 2018-10-07 21:44:02,863 INFO   Validation loss: 3.544 acc: 0.108
13: 2018-10-07 21:44:02,864 INFO Finished training
13: 2018-10-07 21:44:02,864 INFO Train samples 2048 time 101.476s rate 20.1822 samples/s
13: 2018-10-07 21:44:02,864 INFO Valid rate: 95.7245 samples/s
13: 2018-10-07 21:44:02,864 INFO All done!
10: Files already downloaded and verified
10: Files already downloaded and verified
13: Files already downloaded and verified
13: Files already downloaded and verified
 3: 2018-10-07 21:44:03,080 INFO   Validation loss: 3.498 acc: 0.108
 3: 2018-10-07 21:44:03,081 INFO Finished training
 3: 2018-10-07 21:44:03,081 INFO Train samples 2048 time 101.476s rate 20.1821 samples/s
 3: 2018-10-07 21:44:03,081 INFO Valid rate: 95.4973 samples/s
 3: 2018-10-07 21:44:03,081 INFO All done!
 3: Files already downloaded and verified
 3: Files already downloaded and verified
 6: 2018-10-07 21:44:03,256 INFO   Validation loss: 3.580 acc: 0.108
 6: 2018-10-07 21:44:03,256 INFO Finished training
 6: 2018-10-07 21:44:03,257 INFO Train samples 2048 time 101.477s rate 20.182 samples/s
 6: 2018-10-07 21:44:03,257 INFO Valid rate: 95.2842 samples/s
 6: 2018-10-07 21:44:03,257 INFO All done!
 6: Files already downloaded and verified
 6: Files already downloaded and verified
 0: 2018-10-07 21:44:03,539 INFO   Validation loss: 3.559 acc: 0.108
 2: 2018-10-07 21:44:03,570 INFO   Validation loss: 3.544 acc: 0.108
 2: 2018-10-07 21:44:03,570 INFO Finished training
 2: 2018-10-07 21:44:03,570 INFO Train samples 2048 time 101.476s rate 20.1821 samples/s
 2: 2018-10-07 21:44:03,570 INFO Valid rate: 94.9378 samples/s
 2: 2018-10-07 21:44:03,571 INFO All done!
 2: Files already downloaded and verified
 2: Files already downloaded and verified
 0: 2018-10-07 21:44:03,705 INFO Saving summaries to /global/cscratch1/sd/sfarrell/pytorch-cifar10/output/summaries.npz
 0: 2018-10-07 21:44:03,716 INFO Finished training
 0: 2018-10-07 21:44:03,716 INFO Train samples 2048 time 101.474s rate 20.1825 samples/s
 0: 2018-10-07 21:44:03,716 INFO Valid rate: 94.9712 samples/s
 0: 2018-10-07 21:44:03,716 INFO All done!
 0: Files already downloaded and verified
 0: Files already downloaded and verified
